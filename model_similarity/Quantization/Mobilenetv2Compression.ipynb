{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import copy\n",
    "# torch libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "import utils\n",
    "from quant_utils import *\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
    "            # Replace with ReLU\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup, momentum=0.1),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        # Replace torch.add with floatfunctional\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return self.skip_add.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, mean, std = utils.get_subtraining_dataloader_tinyimagenet_intersect(\n",
    "    propor=1.0, \n",
    "    batch_size=128, \n",
    "    num_workers=8, \n",
    "    shuffle=True, \n",
    "    sub_idx=1)\n",
    "test_loader = utils.get_test_dataloader_tinyimagenet(\n",
    "    mean, std, \n",
    "    batch_size=128, num_workers=8, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2(num_classes=200)\n",
    "model.load_state_dict(\n",
    "    torch.load('/data1/checkpoint/hash/tinyimagenet/mobilenet_v2_0.pth', map_location=device))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"Loaded model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_model= copy.deepcopy(model)\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "# The model has to be switched to evaluation mode before any layer fusion.\n",
    "# Otherwise the quantization will not work correctly.\n",
    "fused_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in fused_model.modules():\n",
    "    if type(m) == ConvBNReLU:\n",
    "        torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "    if type(m) == InvertedResidual:\n",
    "        for idx in range(len(m.conv)):\n",
    "            if type(m.conv[idx]) == nn.Conv2d:\n",
    "                torch.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dt/anaconda3/envs/cvnlp/lib/python3.9/site-packages/torch/quantization/observer.py:122: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedNetwork(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (model): MobileNetV2(\n",
       "    (features): Sequential(\n",
       "      (0): ConvBNReLU(\n",
       "        (0): ConvReLU2d(\n",
       "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (1): Identity()\n",
       "        (2): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): Conv2d(\n",
       "            32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): ConvReLU2d(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
       "              (1): ReLU()\n",
       "              (activation_post_process): HistogramObserver()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Identity()\n",
       "          )\n",
       "          (2): Conv2d(\n",
       "            960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "      )\n",
       "      (18): ConvBNReLU(\n",
       "        (0): ConvReLU2d(\n",
       "          (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (1): Identity()\n",
       "        (2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (quant): QuantStub(\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (dequant): DeQuantStub()\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(\n",
       "        in_features=1280, out_features=200, bias=True\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model = QuantizedNetwork(fused_model)\n",
    "quantized_model.eval()\n",
    "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "quantized_model.qconfig = quantization_config\n",
    "print(quantized_model.qconfig)\n",
    "torch.quantization.prepare(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dt/anaconda3/envs/cvnlp/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "/home/dt/anaconda3/envs/cvnlp/lib/python3.9/site-packages/torch/quantization/observer.py:964: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55min, sys: 54.6 s, total: 55min 55s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "calibrate_model(model=quantized_model, loader=train_loader, device='cpu')\n",
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "quantized_model.eval()\n",
    "# Print quantized model.\n",
    "# print(quantized_model)\n",
    "# Save quantized model.\n",
    "save_torchscript_model(model=quantized_model, model_dir='/data1/checkpoint/hash/tinyimagenet/', model_filename=\"mobilenet_v2_0_quant.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model   \t Size (KB): 10161.357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10161357"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model   \t Size (KB): 2906.041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2906041"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 79/79 [00:02<00:00, 39.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 evaluation accuracy: 0.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, int8_eval_accuracy = evaluate_model(model=quantized_model, test_loader=test_loader, device=device, criterion=None)\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 79/79 [00:13<00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 evaluation accuracy: 0.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=None)\n",
    "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f3f0cb6f7dc525fd91e30599dc917c9059637fcae9cfadc503d008ae5db0235"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
